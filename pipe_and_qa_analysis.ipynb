{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eb0ae71-0c67-4c7b-8b3a-c6a1420909bd",
   "metadata": {},
   "source": [
    "This notebook analyze the performance of the deberta model in QA model when Fold 0 is used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183cc605-4fc6-46ac-afd8-fb76631c5777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoModelForQuestionAnswering\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import sys\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a51a9c1-23c8-4096-939f-61b5b57958f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/\"\n",
    "def load_data(fold):\n",
    "    if fold == 0:\n",
    "        dataset = load_dataset('json', data_files={\n",
    "            'train': [data_path + 'subset-0.json', data_path + 'subset-1.json', data_path + 'subset-2.json',\n",
    "                      data_path + 'subset-3.json'],\n",
    "            'validation': data_path + 'subset-4.json'})\n",
    "    elif fold == 1:\n",
    "        dataset = load_dataset('json', data_files={\n",
    "            'train': [data_path + 'subset-1.json', data_path + 'subset-2.json', data_path + 'subset-3.json',\n",
    "                      data_path + 'subset-4.json'],\n",
    "            'validation': data_path + 'subset-0.json'})\n",
    "    elif fold == 2:\n",
    "        dataset = load_dataset('json', data_files={\n",
    "            'train': [data_path + 'subset-0.json', data_path + 'subset-2.json', data_path + 'subset-3.json',\n",
    "                      data_path + 'subset-4.json'\n",
    "                      ],\n",
    "            'validation': data_path + 'subset-1.json'})\n",
    "    elif fold == 3:\n",
    "        dataset = load_dataset('json', data_files={\n",
    "            'train': [data_path + 'subset-0.json', data_path + 'subset-1.json', data_path + 'subset-3.json',\n",
    "                      data_path + 'subset-4.json'],\n",
    "            'validation': data_path + 'subset-2.json'})\n",
    "    elif fold == 4:\n",
    "        dataset = load_dataset('json', data_files={\n",
    "            'train': [data_path + 'subset-0.json', data_path + 'subset-1.json', data_path + 'subset-2.json',\n",
    "                      data_path + 'subset-4.json'],\n",
    "            'validation': data_path + 'subset-3.json'})\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb1ebca-d145-40c4-a964-6ab658628294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change standard output for this notebook\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout= open(\"q3_deberta_fold4_WITHOUT_impossible_answers.txt\",\"w\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdc446f-f99a-49ef-80be-86efa5602033",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exp = 1\n",
    "#load fold 1\n",
    "data_s = load_data(exp)\n",
    "val_data = data_s[\"validation\"]#.select(range(100))\n",
    "\n",
    "extracted_ds = set()\n",
    "idss = list()\n",
    "model_name = \"deberta-base\"\n",
    "\n",
    "q_no = 3\n",
    "#folder_name = f\"analysis_dir/deberta-base-exp{exp}-q{q_no}-analysis\"\n",
    "question = \"Which dataset or database is used?\"\n",
    "message= f\"Here we check the performance of {model_name} with on 10 positive and 10 negative contentexts.#The model from experiment {exp} with question 3 is used.\"\n",
    "#The model from experiment 1 with question q4 (i.q. q3) is used.\"\n",
    "model_checkpoint =f\"best_model/fold{exp}\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "pipe = pipeline(\"question-answering\",model=model,tokenizer = tokenizer,device=0)\n",
    "\n",
    "\n",
    "metric = evaluate.load(\"squad_v2\")\n",
    "\n",
    "predicted_answers = []\n",
    "theoretical_answers = []\n",
    "result = []\n",
    "for sample in tqdm(val_data):\n",
    "    predicted_ans = pipe(question=sample[\"question\"],context=sample[\"context\"])#,handle_impossible_answer=True)\n",
    "\n",
    "    result.append((predicted_ans[\"answer\"],predicted_ans[\"start\"],predicted_ans[\"end\"],\n",
    "                  sample[\"answers\"][\"text\"],sample[\"answers\"][\"answer_start\"],sample[\"id\"]))\n",
    "    if predicted_ans[\"start\"] != predicted_ans[\"end\"]:\n",
    "        #(predicted,start,end,true,start)\n",
    "        predicted_answers.append({\"id\": sample[\"id\"], \"prediction_text\": predicted_ans[\"answer\"].strip(),\"no_answer_probability\": 0.0})\n",
    "    else:\n",
    "        predicted_answers.append({\"id\": sample[\"id\"], \"prediction_text\": \"\",\"no_answer_probability\": 1.0})\n",
    "        \n",
    "    theoretical_answers.append({\"id\": sample[\"id\"], \"answers\": sample[\"answers\"]})\n",
    "\n",
    "#print(predicted_answers))\n",
    "print(extracted_ds)\n",
    "print(\"\\n*****************************************##################$$$$$$$$$$$$$$$$$$$$$$$$$$$\\n\")\n",
    "#print(theoretical_answers)\n",
    "print(len(idss))\n",
    "print(\"__________________________________________________________________________________________\")\n",
    "print(metric.compute(predictions=predicted_answers, references=theoretical_answers))\n",
    "print(\"________________________________________________________________________________________\")\n",
    "#print(match_list)\n",
    "print(\"work is done2222222222222222222222222222222\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fac8185-e15f-459b-b7b0-04a8b40d2b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #This function reads the dataset id file into a dictionary\n",
    "def read_dataset_ids_file(dataset_ids_file):\n",
    "    dic = {}\n",
    "    with open(dataset_ids_file,\"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.split(\":\")\n",
    "            ds_strings = line[1]\n",
    "            ds_strings = ds_strings.split(\"$\")\n",
    "            dic[line[0].strip()] = set([x.strip() for x in ds_strings])\n",
    "\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e259c74c-6e0f-48bf-b99c-b92070489cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function takes a dataset mention string and return its dataset id'''\n",
    "def get_dataset_id(dataset_str,id_dict=None):\n",
    "    if id_dict == None:\n",
    "        id_dict = read_dataset_ids_file()\n",
    "    for k,v in id_dict.items():\n",
    "        if dataset_str in v:\n",
    "            return k\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ca3b0d-4da5-432a-bbbe-db7a1488cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old_stdout = sys.stdout\n",
    "for exp in [1]: #every exp is on one fold\n",
    "    #sys.stdout= open(f\"qa_with_cls_as_probs/fold{exp}_with_impossible.txt\",\"w\")\n",
    "    \n",
    "    #load fold 1\n",
    "    data_s = load_data(exp)\n",
    "    val_data = data_s[\"validation\"]#.select(range(100))\n",
    "\n",
    "    extracted_ds = set()\n",
    "    idss = list()\n",
    "    model_name = \"deberta-base\"\n",
    "\n",
    "    q_no = 3\n",
    "    #folder_name = f\"analysis_dir/deberta-base-exp{exp}-q{q_no}-analysis\"\n",
    "    question = \"Which dataset or database is used?\"\n",
    "    message= f\"Here we check the performance of {model_name} with positive and  negative contentexts.The model from experiment {exp} with question 3 is used with classification as probability for the no_answer\"\n",
    "    #The model from experiment 1 with question q4 (i.q. q3) is used.\"\n",
    "    model_checkpoint =f\"best_model/fold{exp}\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "    pipe = pipeline(\"question-answering\",model=model,tokenizer = tokenizer,device=0)\n",
    "\n",
    "\n",
    "    metric = evaluate.load(\"squad_v2\")\n",
    "\n",
    "    predicted_answers = []\n",
    "    theoretical_answers = []\n",
    "    result = []\n",
    "    for sample in tqdm(val_data):\n",
    "        predicted_ans = pipe(question=sample[\"question\"],context=sample[\"context\"],handle_impossible_answer=True)\n",
    "\n",
    "        result.append((predicted_ans[\"answer\"],predicted_ans[\"start\"],predicted_ans[\"end\"],\n",
    "                      sample[\"answers\"][\"text\"],sample[\"answers\"][\"answer_start\"],sample[\"id\"]))\n",
    "        if predicted_ans[\"start\"] != predicted_ans[\"end\"]:\n",
    "            #(predicted,start,end,true,start)\n",
    "            predicted_answers.append({\"id\": sample[\"id\"], \"prediction_text\": predicted_ans[\"answer\"].strip(),\"no_answer_probability\": 0.0})\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": sample[\"id\"], \"prediction_text\": \"\",\"no_answer_probability\": 1.0})\n",
    "\n",
    "        theoretical_answers.append({\"id\": sample[\"id\"], \"answers\": sample[\"answers\"]})\n",
    "\n",
    "    #print(predicted_answers))\n",
    "    print(extracted_ds)\n",
    "    print(\"\\n*****************************************##################$$$$$$$$$$$$$$$$$$$$$$$$$$$\\n\")\n",
    "    #print(theoretical_answers)\n",
    "    print(len(idss))\n",
    "    print(\"__________________________________________________________________________________________\")\n",
    "    print(metric.compute(predictions=predicted_answers, references=theoretical_answers))\n",
    "    print(\"________________________________________________________________________________________\")\n",
    "    #print(match_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab3cd9-9979-4e75-a82c-ee457cb0c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p positive n negative\n",
    "#first is true and second is predicted\n",
    "tp = []\n",
    "fn = []\n",
    "fp= []\n",
    "tn= []\n",
    "for x in result:                                                \n",
    "    if len(x[3]) >0 and x[0] != '':#tp                               \n",
    "        tp.append(x)\n",
    "    elif len(x[3]) >0 and x[0] == '': #fn                          \n",
    "        fn.append(x)                                                \n",
    "    elif len(x[3]) ==0 and x[0] != '':#fp                           \n",
    "        fp.append(x)                                                \n",
    "    elif len(x[3]) == 0 and x[0] =='':#tn                       \n",
    "        tn.append(x) \n",
    "        \n",
    "print(f\"TP: {len(tp)}, FN: {len(fn)}, FP: {len(fp)}, TN: {len(tn)}\\n\")\n",
    "print(\"**********************************************************************\")\n",
    "\n",
    "print(\"True Positive\\n\")\n",
    "tp_ex = {}\n",
    "for x in tp:\n",
    "    ds_str = x[3][0]\n",
    "    if ds_str not in tp_ex.keys():\n",
    "        tp_ex[ds_str] =1\n",
    "    else:\n",
    "        tp_ex[ds_str] +=1\n",
    "for k,v in tp_ex.items():\n",
    "    print(\"{0},{1}\\n\".format(k,v))\n",
    "print(\"**********************************************************************\") \n",
    "print(\"False Negative\")\n",
    "fn_ex = {}\n",
    "for x in fn:\n",
    "    ds_str = x[3][0]\n",
    "    if ds_str not in fn_ex.keys():\n",
    "        fn_ex[ds_str] =1\n",
    "    else:\n",
    "        fn_ex[ds_str] +=1\n",
    "for k,v in fn_ex.items():\n",
    "    print(\"{0},{1}\\n\".format(k,v))\n",
    "\n",
    "print(\"**********************************************************************\")\n",
    "print(\"False Positive\\n\")\n",
    "fp_ex = {}\n",
    "for x in fp:\n",
    "    print(x)\n",
    "    ds_str = x[0]\n",
    "    if ds_str not in fp_ex.keys():\n",
    "        fp_ex[ds_str] =1\n",
    "    else:\n",
    "        fp_ex[ds_str] +=1\n",
    "for k,v in fp_ex.items():\n",
    "    print(\"{0},{1}\\n\".format(k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a32a6-1760-4a7e-95f9-0d2165e3effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change standard output again\n",
    "sys.stdout = old_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e990b-9051-4c91-a09d-e9fc3da88e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = \"/media/nvme1n1/yyounes/paper_1/data/datasets_final.txt\"\n",
    "\n",
    "ds_dict = read_dataset_ids_file(dataset_file)\n",
    "tp_dict = {}\n",
    "print(len(ds_dict))\n",
    "\n",
    "trash = []\n",
    "for x in tp:\n",
    "    ds_id = get_dataset_id(dataset_str=x[0].strip(),id_dict=ds_dict)\n",
    "    if ds_id != None and int(ds_id) in valid_ds_ids:\n",
    "        if ds_id not in tp_dict:\n",
    "            tp_dict[ds_id]=1\n",
    "        else: #if ds_id != None:\n",
    "            tp_dict[ds_id]+=1\n",
    "    else:\n",
    "        trash.append(ds_id)\n",
    "print(\"Trash ids\")\n",
    "print(trash)\n",
    "print(\"************************\")\n",
    "tp_dict['5'] = 0 #add this because dataset 5 does not appear in the results although it is in the gold standard\n",
    "sorted_dict = {k: v for k, v in sorted(tp_dict.items(), key=lambda item: int(item[0]),reverse=False)}\n",
    "#sorted_dict = dict(sorted(tp_dict.items(),reverse=True))\n",
    "id_list = list(sorted_dict.keys())\n",
    "count_list = list(sorted_dict.values())\n",
    "print(id_list)\n",
    "print(count_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05396e74-532b-4009-8b36-ddcf6710126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in tp:\n",
    "    pred_start = x[1]\n",
    "    gold_start = list(x[4])\n",
    "    print(\"index difference\")\n",
    "    #print(f\"{x[0]}:{gold_start[0]-pred_start}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793423cc-e800-4b5a-a1ad-57e24cbb6cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid_ds_ids=[6, 18, 2, 25, 12, 5, 31, 30]\n",
    "\n",
    "fn_dict = {}\n",
    "for fff in valid_ds_ids:\n",
    "    fn_dict[str(fff)]=0\n",
    "    \n",
    "for x in fn:\n",
    "    ds_str = list(x[3])[0] #get dataset gold standard string\n",
    "    ds_id = get_dataset_id(ds_str,ds_dict)\n",
    "    if ds_id == '18':\n",
    "        print(ds_str)\n",
    "        print(\"****************\")\n",
    "    if ds_id in fn_dict:\n",
    "        fn_dict[ds_id]+=1\n",
    "    #else:\n",
    "        #fn_dict[ds_id]+=1\n",
    "        \n",
    "        \n",
    "#fn_dict['30']=0\n",
    "#fn_dict['31']=0\n",
    "print(fn_dict[str(18)])\n",
    "sorted_dict_fn = {k: v for k, v in sorted(fn_dict.items(), key=lambda item: int(item[0]),reverse=False)}\n",
    "#sorted_dict = dict(sorted(tp_dict.items(),reverse=True))\n",
    "id_list_fn= list(sorted_dict_fn.keys())\n",
    "count_list_fn = list(sorted_dict_fn.values())\n",
    "print(id_list_fn)\n",
    "print(count_list_fn)\n",
    "#count_list_fn = [-1*x for x in count_list_fn]\n",
    "#id_list.append('99-new')\n",
    "#count_list.append(29)\n",
    "print(count_list)\n",
    "print(id_list)\n",
    "original_support = [138,2,13184,107,138,128,1,1]\n",
    "fig, ax = plt.subplots()    \n",
    "width = 0.4 # the width of the bars \n",
    "#ind = np.arange(len(id_list))  # the x locations for the groups\n",
    "#ax.bar(id_list_fn, count_list_fn, width, color=\"orange\")\n",
    "ax.bar(id_list,count_list,width,color=\"blue\")\n",
    "#ax.set_yticks(ind+width/2)\n",
    "#ax.set_yticklabels(x, minor=False)\n",
    "for i, o_sup,c_tp in zip(id_list,original_support,count_list):\n",
    "    if c_tp == 0:\n",
    "        print(i)\n",
    "    lb = f'{c_tp}/{o_sup}'\n",
    "    #ax.text(i, c_fn-500, str(c_fn), color='black', fontweight='bold')#,rotation=90)\n",
    "    ax.text(i,c_tp+15,lb,color='black',fontweight='bold')\n",
    "#for i, v in enumerate(count_list):\n",
    "#    ax.text(v + 3, i, str(v), color='black', fontweight='bold')\n",
    "#plt.title('Datasets Found in Fold 0')\n",
    "plt.ylabel('Number of datasets')\n",
    "plt.xlabel('Dataset ID')      \n",
    "#plt.show()\n",
    "fig.set_size_inches(6, 4)\n",
    "fig.savefig(\"fold0_ids_found.pdf\",dpi=300)\n",
    "#fig.savefig(os.path.join('fold0_ids_found.jpg'), dpi=300, format='pddf', bbox_inches='tight') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
